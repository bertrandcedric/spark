{
  "paragraphs": [
    {
      "text": "%dep\nz.reset()\nz.addRepo(\"Spark Packages Repo\").url(\"http://dl.bintray.com/spark-packages/maven\")\nz.load(\"com.databricks:spark-csv_2.11:1.3.0\")",
      "dateUpdated": "Feb 19, 2016 2:21:52 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455891712468_-557840945",
      "id": "20160219-142152_1656621214",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Must be used before SparkInterpreter (%spark) initialized\nHint: put this paragraph before any Spark code and restart Zeppelin/Interpreter"
      },
      "dateCreated": "Feb 19, 2016 2:21:52 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df \u003d sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"src/main/resources/data/home_data.csv\").cache()\n\ndf.printSchema()\n\nval zipcode \u003d df.select(\"price\", \"zipcode\").groupBy(\"zipcode\").mean(\"price\").sort(desc(\"avg(price)\")).show()\n",
      "dateUpdated": "Feb 19, 2016 2:22:57 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455891712469_-558225694",
      "id": "20160219-142152_1748699829",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "java.lang.ClassNotFoundException: Failed to find data source: com.databricks.spark.csv. Please find packages at http://spark-packages.org\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.lookupDataSource(ResolvedDataSource.scala:77)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:102)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:119)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:109)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:50)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:55)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:57)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:59)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:61)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:63)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:65)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:67)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:69)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:71)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:73)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:75)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:77)\n\tat \u003cinit\u003e(\u003cconsole\u003e:79)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:83)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:709)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:673)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:666)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:295)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:171)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ClassNotFoundException: com.databricks.spark.csv.DefaultSource\n\tat scala.tools.nsc.interpreter.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:83)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$$anonfun$4$$anonfun$apply$1.apply(ResolvedDataSource.scala:62)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$$anonfun$4$$anonfun$apply$1.apply(ResolvedDataSource.scala:62)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$$anonfun$4.apply(ResolvedDataSource.scala:62)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$$anonfun$4.apply(ResolvedDataSource.scala:62)\n\tat scala.util.Try.orElse(Try.scala:82)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.lookupDataSource(ResolvedDataSource.scala:62)\n\t... 46 more\n\n"
      },
      "dateCreated": "Feb 19, 2016 2:21:52 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.commons.io.IOUtils\nimport java.io.File\nimport java.nio.charset.Charset\n\nval bankText \u003d sc.textFile(\"/zeppelin/notebook/people-example.csv\")\n\ncase class People(firstname: String, lastname: String, country: String, age: Integer)\n\nval bank \u003d bankText.map(s \u003d\u003e s.split(\",\")).map(\n    s \u003d\u003e People(s(0), s(1), s(2), s(3).toInt)\n).toDF()\n\nbank.registerTempTable(\"bank\")\n",
      "dateUpdated": "Feb 19, 2016 2:23:33 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455891712469_-558225694",
      "id": "20160219-142152_1402383165",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.commons.io.IOUtils\nimport java.io.File\nimport java.nio.charset.Charset\nbankText: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[20] at textFile at \u003cconsole\u003e:41\ndefined class People\nbank: org.apache.spark.sql.DataFrame \u003d [firstname: string, lastname: string, country: string, age: int]\n"
      },
      "dateCreated": "Feb 19, 2016 2:21:52 PM",
      "dateStarted": "Feb 19, 2016 2:23:33 PM",
      "dateFinished": "Feb 19, 2016 2:23:36 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql \nselect *\nfrom bank ",
      "dateUpdated": "Feb 19, 2016 2:23:38 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "graph": {
          "mode": "pieChart",
          "height": 300.0,
          "optionOpen": true,
          "keys": [
            {
              "name": "country",
              "index": 2.0,
              "aggr": "sum",
              "$$hashKey": "object:2821"
            }
          ],
          "values": [
            {
              "name": "country",
              "index": 2.0,
              "aggr": "count",
              "$$hashKey": "object:2823"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "firstname",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "lastname",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455891712469_-558225694",
      "id": "20160219-142152_284817320",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "firstname\tlastname\tcountry\tage\n\"Bob\"\t\"Smith\"\t\"United States\"\t24\n\"Alice\"\t\"Williams\"\t\"Canada\"\t23\n\"Malcolm\"\t\"Jone\"\t\"England\"\t22\n\"Felix\"\t\"Brown\"\t\"USA\"\t23\n\"Alex\"\t\"Cooper\"\t\"Poland\"\t23\n\"Tod\"\t\"Campbell\"\t\"United States\"\t22\n\"Derek\"\t\"Ward\"\t\"Switzerland\"\t25\n"
      },
      "dateCreated": "Feb 19, 2016 2:21:52 PM",
      "dateStarted": "Feb 19, 2016 2:23:38 PM",
      "dateFinished": "Feb 19, 2016 2:23:39 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Feb 19, 2016 2:21:52 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455891712470_-557071447",
      "id": "20160219-142152_43120593",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Feb 19, 2016 2:21:52 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Getting started Zeppelin",
  "id": "2BDPCUE6N",
  "angularObjects": {
    "2BCSBAKHJ": [],
    "2BCY326GK": [],
    "2BEWC3M2V": [],
    "2BDUX9G5H": [],
    "2BEPZTSFY": [],
    "2BE5QJRCR": [],
    "2BD1JHHBD": [],
    "2BBUHD36G": [],
    "2BEW2685J": [],
    "2BD29H49P": [],
    "2BCSGCDJE": [],
    "2BEE8URH2": [],
    "2BC3F4VNA": [],
    "2BCXWMXXU": [],
    "2BD2EQRJJ": []
  },
  "config": {},
  "info": {}
}